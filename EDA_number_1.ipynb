{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Number 1\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates a clean and properly summarized exploratory data analysis using NumPy and Pandas. The analysis follows best practices for data exploration, cleaning, and visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports\n",
    "Import essential libraries for data manipulation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Overview\n",
    "Load the dataset and perform initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for demonstration\n",
    "# Replace this with your actual data loading: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'score': np.random.uniform(0, 100, n_samples),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_samples),\n",
    "    'is_active': np.random.choice([True, False], n_samples)\n",
    "})\n",
    "\n",
    "# Introduce some missing values for demonstration\n",
    "df.loc[np.random.choice(df.index, 5), 'income'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 3), 'score'] = np.nan\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last few rows\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"Data types of each column:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "Identify and handle data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "print(f\"Percentage of missing values: {(missing_values.sum() / df.size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Columns')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (example: fill with median for numerical columns)\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Fill numerical missing values with median\n",
    "numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df_cleaned[col].isnull().sum() > 0:\n",
    "        median_value = df_cleaned[col].median()\n",
    "        df_cleaned[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median: {median_value:.2f}\")\n",
    "\n",
    "print(f\"\\nMissing values after cleaning: {df_cleaned.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Summary\n",
    "- **Missing Values**: Identified and filled numerical missing values with median\n",
    "- **Duplicates**: Checked for duplicate rows\n",
    "- **Data Types**: Verified all columns have appropriate data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Univariate Analysis\n",
    "Analyze individual variables to understand their distributions and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "print(\"Statistical Summary:\")\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical variables\n",
    "numerical_cols = ['age', 'income', 'score']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].hist(df_cleaned[col], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col.capitalize()}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(col.capitalize())\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].boxplot(df_cleaned[col].dropna(), vert=True)\n",
    "    axes[idx].set_title(f'Box Plot: {col.capitalize()}', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col.capitalize())\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable analysis\n",
    "print(\"Category Distribution:\")\n",
    "category_counts = df_cleaned['category'].value_counts()\n",
    "print(category_counts)\n",
    "print(f\"\\nCategory Percentages:\")\n",
    "print(df_cleaned['category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "df_cleaned['category'].value_counts().plot(kind='bar', ax=axes[0], color='coral', edgecolor='black')\n",
    "axes[0].set_title('Category Distribution (Bar Chart)', fontweight='bold')\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "df_cleaned['category'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Category Distribution (Pie Chart)', fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bivariate and Multivariate Analysis\n",
    "Explore relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "correlation_matrix = df_cleaned[numerical_cols].corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots to visualize relationships\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Age vs Income\n",
    "axes[0].scatter(df_cleaned['age'], df_cleaned['income'], alpha=0.6, color='blue')\n",
    "axes[0].set_title('Age vs Income', fontweight='bold')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Income')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Age vs Score\n",
    "axes[1].scatter(df_cleaned['age'], df_cleaned['score'], alpha=0.6, color='green')\n",
    "axes[1].set_title('Age vs Score', fontweight='bold')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Income vs Score\n",
    "axes[2].scatter(df_cleaned['income'], df_cleaned['score'], alpha=0.6, color='red')\n",
    "axes[2].set_title('Income vs Score', fontweight='bold')\n",
    "axes[2].set_xlabel('Income')\n",
    "axes[2].set_ylabel('Score')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group analysis by category\n",
    "print(\"Statistical Summary by Category:\")\n",
    "grouped_stats = df_cleaned.groupby('category')[numerical_cols].agg(['mean', 'median', 'std'])\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions by category\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    df_cleaned.boxplot(column=col, by='category', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{col.capitalize()} by Category', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Category')\n",
    "    axes[idx].set_ylabel(col.capitalize())\n",
    "    axes[idx].get_figure().suptitle('')  # Remove the automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for comprehensive multivariate analysis\n",
    "print(\"Generating pair plot...\")\n",
    "sns.pairplot(df_cleaned[numerical_cols + ['category']], hue='category', diag_kind='kde', corner=False)\n",
    "plt.suptitle('Pair Plot: Multivariate Analysis', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings and Insights\n",
    "\n",
    "### Data Quality\n",
    "- Dataset contains **100 observations** with **6 variables**\n",
    "- Identified and handled **missing values** in income and score columns\n",
    "- No duplicate records found\n",
    "- All data types are appropriate for analysis\n",
    "\n",
    "### Distribution Insights\n",
    "- **Age**: Uniformly distributed between 18 and 65 years\n",
    "- **Income**: Approximately normally distributed with mean around $50,000\n",
    "- **Score**: Uniformly distributed between 0 and 100\n",
    "- **Category**: Three categories (A, B, C) are fairly balanced\n",
    "\n",
    "### Relationships\n",
    "- Correlation analysis reveals relationships between variables\n",
    "- No strong linear correlations observed in the sample data\n",
    "- Categories show different patterns in the numerical variables\n",
    "\n",
    "### Recommendations for Further Analysis\n",
    "1. **Feature Engineering**: Consider creating derived variables based on existing features\n",
    "2. **Outlier Treatment**: Investigate and handle outliers identified in box plots\n",
    "3. **Advanced Analytics**: Apply statistical tests to validate observed patterns\n",
    "4. **Predictive Modeling**: Use cleaned data for machine learning applications\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "This EDA provides a comprehensive overview of the dataset, following best practices for data exploration. The analysis includes data quality checks, univariate analysis, bivariate relationships, and multivariate patterns. The cleaned dataset is now ready for further statistical analysis or machine learning applications.\n",
    "\n",
    "**Note**: This notebook uses sample data for demonstration. Replace the data loading section with your actual dataset to perform a real analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of EDA_number_1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
